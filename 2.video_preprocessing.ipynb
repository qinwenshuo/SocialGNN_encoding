{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use SocialGNN_encoding_2 conda environment",
   "id": "f28823a88c1caa37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To facilitate graph creation andnhelp standardize graph size, we only kept clips with at least 2 people and a maximum of 5 entities (people + objects)\n",
    "\n",
    " node features were obtained by passing pixel information within that entity’s bounding box through a pretrained VGG19 network30. The output from the penultimate fully connected layer was reduced to 20 dimensions via PCA and this feature vector was appended with the 4D coordinates of the bounding box (representing the location and size of the entity) and a boolean variable denoting whether it was an agent (person) or an object. "
   ],
   "id": "1f80885b89e718b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T15:31:22.867965Z",
     "start_time": "2024-08-26T15:31:21.137033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T15:31:22.883350Z",
     "start_time": "2024-08-26T15:31:22.871496Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_input_path = 'Data/dyad_videos_3000ms'\n",
    "annotation_input_path = 'Data/annotations.csv'\n",
    "patches_output_path = 'Data/preprocess/video_data/'\n",
    "pca_dir = \"Data/preprocess/fitted_PCA\""
   ],
   "id": "4523962ea1630d72",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T15:31:22.899032Z",
     "start_time": "2024-08-26T15:31:22.884584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        pickled = pickle.load(f)\n",
    "    return pickled"
   ],
   "id": "507d9145b1157808",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T15:31:22.914587Z",
     "start_time": "2024-08-26T15:31:22.901329Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ],
   "id": "a77e8f940055842e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CROP OUT IMAGES PATCHES FROM VIDEOS",
   "id": "d81eafeb9c3912a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "annotations = pd.read_csv(annotation_input_path)\n",
    "annotations"
   ],
   "id": "ed4ba0a57372b6c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate over each unique video in the DataFrame\n",
    "for video_name in annotations['video_name'].unique():\n",
    "    # print(\"Processing video:\", video_name)\n",
    "    # Fetch all frames annotations in this video\n",
    "    save_path = os.path.join(patches_output_path, video_name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    video = cv2.VideoCapture(os.path.join(video_input_path, video_name))\n",
    "    video_annotations = annotations[annotations['video_name'] == video_name]\n",
    "    patches = []\n",
    "    annotations_dict = {'labels': [], 'gazes': [], 'frame_numbers': [], \n",
    "                   'left': [], 'right': [], 'top': [], 'bottom': []}\n",
    "    \n",
    "    for current_frame in range(1, 91):\n",
    "        successful_read, frame = video.read()\n",
    "        if successful_read:\n",
    "            # Filter annotations for the current frame\n",
    "            frame_annotations = video_annotations[video_annotations['frame'] == current_frame]\n",
    "            if not frame_annotations.empty:\n",
    "                for _, entity in frame_annotations.iterrows():\n",
    "                    patches.append(frame[int(entity['top']):int(entity['bottom']),int(entity['left']):int(entity['right'])])\n",
    "                    annotations_dict['labels'].append(entity['label_name'])\n",
    "                    annotations_dict['gazes'].append(entity['gaze_direction'])   \n",
    "                    annotations_dict['frame_numbers'].append(current_frame)\n",
    "                    annotations_dict['left'].append(int(entity['left']))\n",
    "                    annotations_dict['right'].append(int(entity['right']))\n",
    "                    annotations_dict['top'].append(int(entity['top']))\n",
    "                    annotations_dict['bottom'].append(int(entity['bottom']))\n",
    "                    \n",
    "        else:\n",
    "            raise ValueError(f\"Unsuccessful read frame {current_frame} of {video_name}\")\n",
    "    save_pickle(patches, os.path.join(save_path, 'patches'))\n",
    "    annotation_df = pd.DataFrame(annotations_dict)\n",
    "    save_pickle(annotation_df, os.path.join(save_path, 'annotations'))\n"
   ],
   "id": "839f6eaa6f6c797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_patches(num_patches_to_display=10):\n",
    "    # Get a list of all subdirectories in the patches_output_path directory\n",
    "    videos = [d for d in os.listdir(patches_output_path)]\n",
    "    import random\n",
    "    # Randomly select one of the subdirectories\n",
    "    selected_subdir = random.choice(videos)\n",
    "    selected_path = os.path.join(patches_output_path, selected_subdir)\n",
    "    \n",
    "    patches = load_pickle(os.path.join(selected_path, 'patches'))\n",
    "    annot = load_pickle(os.path.join(selected_path, 'annotations'))\n",
    "    \n",
    "    # Display each patch with its corresponding labels\n",
    "    print(len(patches))\n",
    "    for i, patch in enumerate(patches):\n",
    "        print(annot.loc[i, 'frame_numbers'])\n",
    "        print(annot.loc[i, 'labels'])\n",
    "        print(annot.loc[i, 'gazes'])\n",
    "        plt.imshow(cv2.cvtColor(patch, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        if i+2> num_patches_to_display:\n",
    "            break\n"
   ],
   "id": "614aeeb4656355de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_patches(5)",
   "id": "5d4443e98b7be07e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## VGG FEATURES",
   "id": "b183e063371ba14e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import smart_resize\n",
    "from tensorflow.keras.models import Model"
   ],
   "id": "2502b62a7c8fa88e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def reshape_patches(x):\n",
    "    temp = np.expand_dims(x, axis=0)\n",
    "    temp2 = preprocess_input(smart_resize(temp, (224,224)))\n",
    "    return temp2[0]"
   ],
   "id": "21021c03a1c03b04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_model = VGG19(weights='imagenet')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)"
   ],
   "id": "dacc0086384159e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for video in tqdm(os.listdir(patches_output_path)):\n",
    "    video_dir = os.path.join(patches_output_path, video)\n",
    "    patch_dir = os.path.join(video_dir, \"patches\")\n",
    "    out_dir = os.path.join(video_dir, \"VGG19_patches\")\n",
    "    if not os.path.exists(out_dir):\n",
    "        patches = load_pickle(patch_dir)\n",
    "        reshaped_patches  = [reshape_patches(patch) for patch in patches]\n",
    "        x = np.array(reshaped_patches)\n",
    "        y = model.predict(x)\n",
    "        save_pickle(y, out_dir)\n",
    "        print(f\"VGG19 patches saved to {video_dir}\")\n"
   ],
   "id": "6ffaa5d5aa9cb23d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## fit PCA",
   "id": "514469f80c7c4205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "8d81af7e2221d4d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_all_vggfeatures():\n",
    "    all_features = []\n",
    "    for video in tqdm(os.listdir(patches_output_path)):\n",
    "        patch_dir = os.path.join(patches_output_path, video, \"VGG19_patches\") \n",
    "        all_features.extend(load_pickle(patch_dir))\n",
    "        \n",
    "    all_features = np.array(all_features)\n",
    "    print(all_features.shape)\n",
    "    return all_features"
   ],
   "id": "e99f00a3f110c341",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fit_pca(all_features):\n",
    "    pca = PCA(n_components=20)\n",
    "    scaler = StandardScaler()\n",
    "    all_features_scaled = scaler.fit_transform(all_features)\n",
    "    pca.fit(all_features_scaled)\n",
    "    return pca, scaler"
   ],
   "id": "120ec51d62ac3b21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "pca, scaler = fit_pca(extract_all_vggfeatures())\n",
    "save_pickle(pca, os.path.join(pca_dir, \"pca\"))\n",
    "save_pickle(scaler, os.path.join(pca_dir, \"scaler\"))"
   ],
   "id": "87f2c60bd7753554",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PCA on VGG features",
   "id": "6b0ea8004ff8b20b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca = load_pickle(os.path.join(pca_dir, \"pca\"))\n",
    "scaler = load_pickle(os.path.join(pca_dir, 'scaler'))"
   ],
   "id": "cb233f4154b48a2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for video in tqdm(os.listdir(patches_output_path)):\n",
    "    video_dir = os.path.join(patches_output_path, video)\n",
    "    patch_dir = os.path.join(video_dir, \"VGG19_patches\")\n",
    "    vgg_features = load_pickle(patch_dir)\n",
    "    scaled_features = scaler.transform(vgg_features)\n",
    "    pca_features = pca.transform(scaled_features)\n",
    "    save_pickle(pca_features, os.path.join(video_dir, \"pca_features\"))    "
   ],
   "id": "acf85632d92558a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Node Features\n",
   "id": "5ee3a6656d714375"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for video in tqdm(os.listdir(patches_output_path)):\n",
    "    video_dir = os.path.join(patches_output_path, video)\n",
    "    pca_features = load_pickle(os.path.join(video_dir, \"pca_features\"))\n",
    "    video_annot = load_pickle(os.path.join(video_dir, \"annotations\"))\n",
    "    new_features = []\n",
    "    for i, patch_feature in enumerate(pca_features):\n",
    "        new_feature = np.append(patch_feature, [video_annot['left'][i], video_annot['top'][i], video_annot['right'][i], video_annot['bottom'][i]])\n",
    "        new_feature = np.append(new_feature, [0] if video_annot['labels'][i] in ['head1', 'head2'] else [1])\n",
    "        assert len(new_feature) == 25\n",
    "        new_features.append(new_feature)\n",
    "    video_annot['features'] = new_features\n",
    "    save_pickle(video_annot, os.path.join(video_dir, \"annotations\"))"
   ],
   "id": "2fe6b2ee65ec090c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b5c54bde04ea5e5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Graph",
   "id": "ca71efec9cf9e040"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T15:32:04.668641Z",
     "start_time": "2024-08-26T15:31:26.274532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_videos = []\n",
    "all_labels = ['head1', 'head2', 'object1', 'object2', 'placeholder']\n",
    "for video_name in tqdm(sorted(os.listdir(patches_output_path))):\n",
    "    video_dict = {'label':[], 'graph_dicts':[], 'video_name': video_name} ## label is not used\n",
    "    video_dir = os.path.join(patches_output_path, video_name)\n",
    "    annotations = load_pickle(os.path.join(video_dir, \"annotations\"))\n",
    "    \n",
    "    grouped = annotations.groupby('frame_numbers')\n",
    "\n",
    "    for frame_number, group in grouped:\n",
    "        edges = group['gazes']\n",
    "        senders = []\n",
    "        receivers = []\n",
    "        nodes = []\n",
    "        # Process in the order of the all_labels list\n",
    "        for i, label in enumerate(all_labels):                \n",
    "            # if the entity exists\n",
    "            if (annotations['labels'] == label).any():\n",
    "                # Access the value in the 'features' and 'gazes' columns of that entity\n",
    "                feature = annotations.loc[annotations['labels'] == label, 'features'].iloc[0]\n",
    "                nodes.append(feature.tolist())\n",
    "                edge = annotations.loc[annotations['labels'] == label, 'gazes'].iloc[0]\n",
    "                # if gaze exists (only when the entity is head1 or head2)\n",
    "                if not isinstance(edge, float):\n",
    "                    sender, receiver = edge.split(', ')\n",
    "                    # if the gaze is at some entities not found in the video, the gaze will be discarded\n",
    "                    if sender == label and receiver != \"neither\" and receiver in all_labels: \n",
    "                        senders.append(i)\n",
    "                        receivers.append(all_labels.index(receiver))\n",
    "            else:\n",
    "                # Pad the nodes if there are missing feature/less than 5 features\n",
    "                nodes.append([0 for j in range(25)])\n",
    "\n",
    "        assert len(nodes) == 5\n",
    "\n",
    "        graph_dict = {'nodes': nodes, 'senders': senders, 'receivers': receivers}\n",
    "        video_dict['graph_dicts'].append(graph_dict)\n",
    "        \n",
    "    all_videos.append(video_dict)"
   ],
   "id": "6bf2a5df40fb0548",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:38<00:00,  6.52it/s]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-26T15:32:04.842487Z",
     "start_time": "2024-08-26T15:32:04.669645Z"
    }
   },
   "cell_type": "code",
   "source": "save_pickle(all_videos, \"Data/preprocess/graphs\")",
   "id": "2e95a494fb3eb82",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "dictionary structure:\n",
    " video index(keys) --> sequences (index) --> frames (index) --> nodes & senders & receivers\n",
    " \n",
    "    ## outer loops: video --> multiple sequences --> multiple frames\n",
    "    ## Under one frame: feature numbers, senders and reveicers in all edges.\n",
    "    print(len(V[key]['graph_dicts'][0][0]['nodes']), len(V[key]['graph_dicts'][0][0]['senders']), len(V[key]['graph_dicts'][0][0]['receivers']))\n",
    "    ## number of frames in a sequence\n",
    "    print(len(V[key]['graph_dicts'][0]))\n",
    "    ## number of sequences in a video\n",
    "    print(len(V[key]['graph_dicts']))\n",
    "    \n",
    "  \n",
    "I guess let me try not to divide the sequences first\n",
    "so I would have \n",
    "\n",
    "video index (keys) --> frames(index) --> nodes & senders & receivers\n",
    "\n",
    "I also didn't do bootstrapping"
   ],
   "id": "b9b8252251365b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c80b0cbe9e904c83",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
