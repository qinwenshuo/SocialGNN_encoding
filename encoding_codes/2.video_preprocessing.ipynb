{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use SocialGNN_encoding_2 conda environment",
   "id": "f28823a88c1caa37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To facilitate graph creation andnhelp standardize graph size, we only kept clips with at least 2 people and a maximum of 5 entities (people + objects)\n",
    "\n",
    " node features were obtained by passing pixel information within that entity’s bounding box through a pretrained VGG19 network30. The output from the penultimate fully connected layer was reduced to 20 dimensions via PCA and this feature vector was appended with the 4D coordinates of the bounding box (representing the location and size of the entity) and a boolean variable denoting whether it was an agent (person) or an object. "
   ],
   "id": "1f80885b89e718b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:04:43.319373Z",
     "start_time": "2024-10-03T18:04:41.872686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:04:43.615623Z",
     "start_time": "2024-10-03T18:04:43.609621Z"
    }
   },
   "cell_type": "code",
   "source": "n_components = 20",
   "id": "f42bc24f82e6a4af",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:04:44.899141Z",
     "start_time": "2024-10-03T18:04:44.881770Z"
    }
   },
   "cell_type": "code",
   "source": [
    "video_input_path = '../Data/dyad_videos_3000ms'\n",
    "annotation_input_path = '../Data/preprocess/annotations.csv'\n",
    "behavioral_ratings_path = '../Data/behavioral_ratings.csv'\n",
    "patches_output_path = '../Data/preprocess/video_data/'\n",
    "pca_dir = \"../Data/preprocess/fitted_PCA\""
   ],
   "id": "4523962ea1630d72",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:04:49.482236Z",
     "start_time": "2024-10-03T18:04:49.464154Z"
    }
   },
   "cell_type": "code",
   "source": "ratings_of_interest =['spatial expanse', 'object directed','interagent distance', 'agents facing', 'communication',  'joint action', 'valence', 'arousal']",
   "id": "da8c8ae32d50bb0d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:04:50.403872Z",
     "start_time": "2024-10-03T18:04:50.390093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        pickled = pickle.load(f)\n",
    "    return pickled"
   ],
   "id": "507d9145b1157808",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:04:51.399718Z",
     "start_time": "2024-10-03T18:04:51.388185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ],
   "id": "a77e8f940055842e",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CROP OUT IMAGES PATCHES FROM VIDEOS",
   "id": "d81eafeb9c3912a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "annotations = pd.read_csv(annotation_input_path)\n",
    "annotations"
   ],
   "id": "ed4ba0a57372b6c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "behavioral_ratings = pd.read_csv(behavioral_ratings_path)\n",
    "behavioral_ratings[behavioral_ratings['video_name']=='-YwZOeyAQC8_15.mp4'][ratings_of_interest].values.tolist()[0]"
   ],
   "id": "e0f61e585fd692a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Iterate over each unique video in the DataFrame\n",
    "for video_name in tqdm(annotations['video_name'].unique()):\n",
    "    # print(\"Processing video:\", video_name)\n",
    "    # Get ratings from this video\n",
    "    video_ratings = behavioral_ratings[behavioral_ratings['video_name']==video_name][ratings_of_interest].values.tolist()[0]\n",
    "\n",
    "    # Fetch all frames annotations in this video\n",
    "    save_path = os.path.join(patches_output_path, video_name)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    video = cv2.VideoCapture(os.path.join(video_input_path, video_name))\n",
    "    video_annotations = annotations[annotations['video_name'] == video_name]\n",
    "    patches = []\n",
    "    annotations_dict = {'labels': [], 'gazes': [], 'frame_numbers': [], \n",
    "                   'left': [], 'right': [], 'top': [], 'bottom': []}\n",
    "    \n",
    "    for current_frame in range(1, 91):\n",
    "        successful_read, frame = video.read()\n",
    "        if successful_read:\n",
    "            # Filter annotations for the current frame\n",
    "            frame_annotations = video_annotations[video_annotations['frame'] == current_frame]\n",
    "            if not frame_annotations.empty:\n",
    "                for _, entity in frame_annotations.iterrows():\n",
    "                    patches.append(frame[int(entity['top']):int(entity['bottom']),int(entity['left']):int(entity['right'])])\n",
    "                    annotations_dict['labels'].append(entity['label_name'])\n",
    "                    annotations_dict['gazes'].append(entity['gaze_direction'])   \n",
    "                    annotations_dict['frame_numbers'].append(current_frame)\n",
    "                    annotations_dict['left'].append(int(entity['left']))\n",
    "                    annotations_dict['right'].append(int(entity['right']))\n",
    "                    annotations_dict['top'].append(int(entity['top']))\n",
    "                    annotations_dict['bottom'].append(int(entity['bottom']))\n",
    "                    \n",
    "        else:\n",
    "            raise ValueError(f\"Unsuccessful read frame {current_frame} of {video_name}\")\n",
    "    save_pickle(patches, os.path.join(save_path, 'patches'))\n",
    "    annotation_df = pd.DataFrame(annotations_dict)\n",
    "    save_pickle(annotation_df, os.path.join(save_path, 'annotations'))\n",
    "    save_pickle(video_ratings, os.path.join(save_path, 'ratings'))\n"
   ],
   "id": "839f6eaa6f6c797",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_patches(num_patches_to_display=10):\n",
    "    # Get a list of all subdirectories in the patches_output_path directory\n",
    "    videos = [d for d in os.listdir(patches_output_path)]\n",
    "    import random\n",
    "    # Randomly select one of the subdirectories\n",
    "    selected_subdir = random.choice(videos)\n",
    "    selected_path = os.path.join(patches_output_path, selected_subdir)\n",
    "    \n",
    "    patches = load_pickle(os.path.join(selected_path, 'patches'))\n",
    "    annot = load_pickle(os.path.join(selected_path, 'annotations'))\n",
    "    \n",
    "    # Display each patch with its corresponding labels\n",
    "    print(len(patches))\n",
    "    for i, patch in enumerate(patches):\n",
    "        print(annot.loc[i, 'frame_numbers'])\n",
    "        print(annot.loc[i, 'labels'])\n",
    "        print(annot.loc[i, 'gazes'])\n",
    "        plt.imshow(cv2.cvtColor(patch, cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        if i+2> num_patches_to_display:\n",
    "            break\n"
   ],
   "id": "614aeeb4656355de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "visualize_patches(5)",
   "id": "5d4443e98b7be07e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## VGG FEATURES",
   "id": "b183e063371ba14e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import smart_resize\n",
    "from tensorflow.keras.models import Model"
   ],
   "id": "2502b62a7c8fa88e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def reshape_patches(x):\n",
    "    temp = np.expand_dims(x, axis=0)\n",
    "    temp2 = preprocess_input(smart_resize(temp, (224,224)))\n",
    "    return temp2[0]"
   ],
   "id": "21021c03a1c03b04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_model = VGG19(weights='imagenet')\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)"
   ],
   "id": "dacc0086384159e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "restart = input('Do you want to reprocess the input videos? (y/n)')",
   "id": "88e40c6e6ac4dcba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for video in tqdm(os.listdir(patches_output_path)):\n",
    "    video_dir = os.path.join(patches_output_path, video)\n",
    "    patch_dir = os.path.join(video_dir, \"patches\")\n",
    "    out_dir = os.path.join(video_dir, \"VGG19_patches\")\n",
    "    if not os.path.exists(out_dir) or restart == 'y':\n",
    "        patches = load_pickle(patch_dir)\n",
    "        reshaped_patches  = [reshape_patches(patch) for patch in patches]\n",
    "        x = np.array(reshaped_patches)\n",
    "        y = model.predict(x)\n",
    "        save_pickle(y, out_dir)\n",
    "        # print(f\"VGG19 patches saved to {video_dir}\")\n"
   ],
   "id": "6ffaa5d5aa9cb23d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## fit PCA",
   "id": "514469f80c7c4205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "8d81af7e2221d4d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_all_vggfeatures():\n",
    "    all_features = []\n",
    "    for video in tqdm(os.listdir(patches_output_path)):\n",
    "        patch_dir = os.path.join(patches_output_path, video, \"VGG19_patches\") \n",
    "        all_features.extend(load_pickle(patch_dir))\n",
    "        \n",
    "    all_features = np.array(all_features)\n",
    "    print(all_features.shape)\n",
    "    return all_features"
   ],
   "id": "e99f00a3f110c341",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fit_pca(all_features):\n",
    "    pca = PCA(n_components=20)\n",
    "    scaler = StandardScaler()\n",
    "    all_features_scaled = scaler.fit_transform(all_features)\n",
    "    pca.fit(all_features_scaled)\n",
    "    return pca, scaler"
   ],
   "id": "120ec51d62ac3b21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca, scaler = fit_pca(extract_all_vggfeatures())\n",
    "save_pickle(pca, os.path.join(pca_dir, f\"{n_components}pca\"))\n",
    "save_pickle(scaler, os.path.join(pca_dir, f\"{n_components}scaler\"))"
   ],
   "id": "87f2c60bd7753554",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Assuming your VGG feature matrix is called vgg_features (shape: [n_samples, n_features])\n",
    "# vgg_features = extract_all_vggfeatures()\n",
    "# \n",
    "# # Apply PCA\n",
    "# pca = PCA()\n",
    "# pca.fit(vgg_features)\n",
    "# \n",
    "# # Get the explained variance ratio for each principal component\n",
    "# explained_variance_ratio = pca.explained_variance_ratio_\n",
    "# \n",
    "# # Calculate the cumulative explained variance\n",
    "# cumulative_variance_explained = np.cumsum(explained_variance_ratio)\n",
    "# \n",
    "# # Plot the cumulative explained variance\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.plot(cumulative_variance_explained, marker='o')\n",
    "# plt.xlabel('Number of Principal Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.title('Variance Explained by PCA Components')\n",
    "# plt.grid(True)\n",
    "# \n",
    "# from kneed import KneeLocator  # External library for detecting the elbow\n",
    "# # Detect the elbow point\n",
    "# kneedle = KneeLocator(range(1, len(cumulative_variance_explained) + 1), \n",
    "#                       cumulative_variance_explained, \n",
    "#                       curve='concave', \n",
    "#                       direction='increasing')\n",
    "# \n",
    "# elbow_point = kneedle.elbow\n",
    "# plt.axvline(x=elbow_point, color='r', linestyle='--', label=f'Elbow at {elbow_point}')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "# \n",
    "# # Print the elbow point\n",
    "# print(f'The elbow is located at component {elbow_point}')\n",
    "# \n",
    "# \n",
    "# plt.show()\n"
   ],
   "id": "84c39cbd100b316e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f7c80f2d37f5f67e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Find the number of dimensions that explain 90% to 95% variance\n",
    "# min_dim_90 = np.argmax(cumulative_variance_explained >= 0.90) + 1  # Adding 1 to get the actual number of components\n",
    "# max_dim_95 = np.argmax(cumulative_variance_explained >= 0.95) + 1  # Adding 1 to get the actual number of components\n",
    "# min_dim_75 = np.argmax(cumulative_variance_explained >= 0.75) + 1\n",
    "# # Print the results\n",
    "# print(f'Number of dimensions that explain at least 90% variance: {min_dim_90}')\n",
    "# print(f'Number of dimensions that explain at least 95% variance: {max_dim_95}')\n",
    "# \n",
    "# print(f'Number of dimensions that explain at least 75% variance: {min_dim_75}')"
   ],
   "id": "4a0be5bfc47da5eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PCA on VGG features",
   "id": "6b0ea8004ff8b20b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca = load_pickle(os.path.join(pca_dir, f\"{n_components}pca\"))\n",
    "scaler = load_pickle(os.path.join(pca_dir, f'{n_components}scaler'))"
   ],
   "id": "cb233f4154b48a2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for video in tqdm(os.listdir(patches_output_path)):\n",
    "    video_dir = os.path.join(patches_output_path, video)\n",
    "    patch_dir = os.path.join(video_dir, \"VGG19_patches\")\n",
    "    vgg_features = load_pickle(patch_dir)\n",
    "    scaled_features = scaler.transform(vgg_features)\n",
    "    pca_features = pca.transform(scaled_features)\n",
    "    save_pickle(pca_features, os.path.join(video_dir, f\"{n_components}pca_features\"))    "
   ],
   "id": "acf85632d92558a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Node Features\n",
   "id": "5ee3a6656d714375"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for video in tqdm(os.listdir(patches_output_path)):\n",
    "    video_dir = os.path.join(patches_output_path, video)\n",
    "    pca_features = load_pickle(os.path.join(video_dir, f\"{n_components}pca_features\"))\n",
    "    video_annot = load_pickle(os.path.join(video_dir, \"annotations\"))\n",
    "    new_features = []\n",
    "    for i, patch_feature in enumerate(pca_features):\n",
    "        new_feature = np.append(patch_feature, [video_annot['top'][i], video_annot['bottom'][i], video_annot['left'][i], video_annot['right'][i]])\n",
    "        new_feature = np.append(new_feature, [0] if video_annot['labels'][i] in ['head1', 'head2'] else [1])\n",
    "        assert len(new_feature) == n_components + 5\n",
    "        new_features.append(new_feature)\n",
    "    video_annot['features'] = new_features\n",
    "    save_pickle(video_annot, os.path.join(video_dir, \"annotations\"))"
   ],
   "id": "2fe6b2ee65ec090c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "video_annot",
   "id": "b5c54bde04ea5e5f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split into Sequences\n",
   "id": "474f1d6acbc9d89f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T15:15:55.448250Z",
     "start_time": "2024-10-03T15:15:55.433457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_sequences(dir, df):\n",
    "    splits = []\n",
    "    start_index = 0\n",
    "    \n",
    "    # Get the unique frame numbers\n",
    "    unique_frames = df['frame_numbers'].unique()\n",
    "    \n",
    "    # Create a pair for the first frame\n",
    "    frame_labels = df[df['frame_numbers'] == unique_frames[0]]['labels'].tolist()\n",
    "    \n",
    "    # Copy the list using list slicing\n",
    "    previous_pairs = frame_labels[:]\n",
    "    print(previous_pairs)\n",
    "    \n",
    "    for i in range(1, len(unique_frames)):\n",
    "        frame = unique_frames[i]\n",
    "        # Create a pair for the current frame, handling NaNs        \n",
    "        frame_labels = df[df['frame_numbers'] == frame]['labels'].tolist()\n",
    "        current_pairs = frame_labels[:]\n",
    "        \n",
    "        # Check if there's any difference in pairs\n",
    "        if current_pairs != previous_pairs:\n",
    "            \n",
    "            # Update the previous pairs\n",
    "            # Sequence must have at least two people\n",
    "            if 'head1' in previous_pairs and 'head2' in previous_pairs:\n",
    "                splits.append(df[df['frame_numbers'].isin(unique_frames[start_index:i])])\n",
    "            previous_pairs = current_pairs[:]\n",
    "            print(previous_pairs, i)\n",
    "            start_index = i\n",
    "\n",
    "    # Append the last segment\n",
    "    if 'head1' in previous_pairs and 'head2' in previous_pairs:\n",
    "        splits.append(df[df['frame_numbers'].isin(unique_frames[start_index:])])\n",
    "    # Save all the sequences\n",
    "    for i, split in enumerate(splits):\n",
    "        save_pickle(split, os.path.join(dir, f\"sequence_{i}\"))"
   ],
   "id": "f9bc19c496d8f547",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T15:15:55.995238Z",
     "start_time": "2024-10-03T15:15:55.949988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for video in tqdm(os.listdir(patches_output_path)):\n",
    "    video_dir = os.path.join(patches_output_path, video)\n",
    "    video_annot = load_pickle(os.path.join(video_dir, \"annotations\"))\n",
    "    split_sequences(video_dir, video_annot)\n",
    "    break"
   ],
   "id": "b834bf024d07ee2b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['head1', 'head2', 'object1']\n",
      "['head1', 'object1'] 57\n",
      "['head1', 'head2', 'object1'] 61\n",
      "['head1', 'object1'] 69\n",
      "['head1', 'head2', 'object1'] 73\n",
      "['head1', 'object1'] 85\n",
      "['head1', 'head2', 'object1'] 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build Graph",
   "id": "ca71efec9cf9e040"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:03.783080Z",
     "start_time": "2024-10-03T18:05:03.768184Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_sequence(sequence_annotations):\n",
    "    \n",
    "    all_labels = ['head1', 'head2', 'object1', 'object2']\n",
    "    sequence_labels = sequence_annotations['labels'].unique().tolist()\n",
    "    # Process in the order of the all_labels list\n",
    "    entities = []\n",
    "    for entity in all_labels:\n",
    "        if entity in sequence_labels:\n",
    "            entities.append(entity)\n",
    "\n",
    "    grouped = sequence_annotations.groupby('frame_numbers')\n",
    "    graph_dicts_frames = []\n",
    "    for frame_number, group in grouped:\n",
    "        senders, receivers, nodes = [], [], []\n",
    "        # print(entities)\n",
    "        for i, entity in enumerate(entities):\n",
    "            # Access the value in the 'features' and 'gazes' columns of that entity\n",
    "            feature = group.loc[group['labels'] == entity, 'features'].iloc[0].tolist()\n",
    "\n",
    "                \n",
    "            nodes.append(feature)\n",
    "            edge = group.loc[group['labels'] == entity, 'gazes'].iloc[0]\n",
    "            # if gaze exists (only when the entity is person)\n",
    "            if not isinstance(edge, float):\n",
    "                sender, receiver = edge.split(', ')\n",
    "                # if the gaze is at some entities not found in the video, the gaze will be discarded\n",
    "                if sender == entity and receiver in entities and sender != receiver: \n",
    "                    senders.append(i)\n",
    "                    receivers.append(entities.index(receiver))\n",
    "                # else:\n",
    "                #     print(f'unrecognised gaze {edge} in {entities}')\n",
    "        while len(nodes) < 4:\n",
    "            nodes.append([0 for _ in range(n_components+5)])\n",
    "            \n",
    "        assert len(nodes) == 4\n",
    "        graph_dict = {'nodes': nodes, 'senders': senders, 'receivers': receivers}\n",
    "        graph_dicts_frames.append(graph_dict)\n",
    "    return graph_dicts_frames"
   ],
   "id": "7cf06ada8f600e6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:22.858141Z",
     "start_time": "2024-10-03T18:05:06.433278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_videos = {}\n",
    "for video_name in tqdm(sorted(os.listdir(patches_output_path))):\n",
    "    # if video_name != '29.mp4':\n",
    "    #     continue\n",
    "    video_dir = os.path.join(patches_output_path, video_name)\n",
    "    video_data = {'graph_dicts': [],\n",
    "                  # 'sequences': [],\n",
    "                  'labels': load_pickle(os.path.join(video_dir, \"ratings\"))}    \n",
    "    num_seq = len([s for s in os.listdir(video_dir) if s.startswith('sequence_')])\n",
    "    for pickle_idx in range(num_seq):\n",
    "        pickle_name = f'sequence_{str(pickle_idx)}'\n",
    "        \n",
    "        sequence_annotations = load_pickle(os.path.join(video_dir, pickle_name))\n",
    "        # if video_name == '144.mp4' and pickle_idx == 2:\n",
    "        #     print(sequence_annotations['labels'])\n",
    "        #     raise ValueError\n",
    "        graph_dicts_sequence = process_sequence(sequence_annotations)\n",
    "        video_data['graph_dicts'].append(graph_dicts_sequence)\n",
    "    all_videos[video_name.replace('.mp4', '')] = video_data\n",
    "    # if video_name == '29.mp4':\n",
    "    #     print(graph_dicts_sequences[0])"
   ],
   "id": "6bf2a5df40fb0548",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:16<00:00, 15.25it/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:22.936218Z",
     "start_time": "2024-10-03T18:05:22.864145Z"
    }
   },
   "cell_type": "code",
   "source": "save_pickle(all_videos, \"../Data/preprocess/graphs\")",
   "id": "2e95a494fb3eb82",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Input Structure",
   "id": "961d8297df41dd62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:24.798291Z",
     "start_time": "2024-10-03T18:05:24.312109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_videos = load_pickle(\"../Data/preprocess/graphs\")\n",
    "structured_graph_dir = '../Data/preprocess/structured_graph'"
   ],
   "id": "268d2b03148aa56d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T18:05:26.851253Z",
     "start_time": "2024-10-03T18:05:26.781089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "structured_graph = []\n",
    "for video_name in tqdm(all_videos.keys()):\n",
    "    for seq_idx, seq in enumerate(all_videos[video_name]['graph_dicts']):\n",
    "        seq_dict = {'label': all_videos[video_name]['labels'],\n",
    "                    'graph_dicts': all_videos[video_name]['graph_dicts'][seq_idx]}\n",
    "        structured_graph.append(seq_dict)\n",
    "save_pickle(structured_graph, structured_graph_dir)"
   ],
   "id": "4629e4516eef8be8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Bootstrapping",
   "id": "4ba38c4df7c788eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# from sklearn.model_selection import train_test_split",
   "id": "742b9e527653564",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# all_videos = load_pickle(\"../Data/preprocess/graphs\")\n",
    "# bootstrapping_dir = '../Data/preprocess/bootstrapped'\n",
    "# bootstrapping = 10"
   ],
   "id": "f8486d19cd9b25aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# myKeys = list(all_videos.keys())\n",
    "# myKeys.sort()\n",
    "# all_videos.keys()"
   ],
   "id": "b7888a80639969fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for date in ['27Sep']:\n",
    "#     ran_state = 13 if date =='27Sep' else 27\n",
    "#     for i in range(bootstrapping):\n",
    "#         all_sequences = []\n",
    "#         seq_train_idx = []\n",
    "#         seq_test_idx = []\n",
    "#         V_train_idx, V_test_idx = train_test_split(list(all_videos.keys()), random_state=ran_state+i)\n",
    "#         print(\"Train Videos\", len(V_train_idx), \"Test Videos\", len(V_test_idx))\n",
    "#         for video_idx in V_train_idx:\n",
    "#             ## return a list of tuple, triple, ... each of them is a sequence event label\n",
    "#             ## For example: [('SingleGaze', 'SingleGaze'), ('GazeFollow', 'GazeFollow')]\n",
    "#             for seq_idx, seq in enumerate(all_videos[video_idx]['graph_dicts']):\n",
    "#                 seq_dict = {'label': all_videos[video_idx]['labels'],\n",
    "#                             'graph_dicts': all_videos[str(video_idx)]['graph_dicts'][seq_idx]}\n",
    "#                 seq_train_idx.append(len(all_sequences))\n",
    "#                 all_sequences.append(seq_dict)\n",
    "#                     # if len(all_sequences) == 312+1 and i == 0:\n",
    "#                     #     print(len(seq_labels))\n",
    "#                     #     print(video_idx, seq_idx)\n",
    "#                     #     print(all_videos[str(video_idx)]['graph_dicts'][seq_idx][0])\n",
    "#                     \n",
    "#         for video_idx in V_test_idx:\n",
    "#             for seq_idx, seq in enumerate(all_videos[video_idx]['graph_dicts']):\n",
    "#                 seq_dict = {'label': seq,\n",
    "#                             'graph_dicts': all_videos[video_idx]['graph_dicts'][seq_idx]}\n",
    "#                 seq_test_idx.append(len(all_sequences))\n",
    "#                 all_sequences.append(seq_dict)\n",
    "#                     \n",
    "#         print(\"Train Seqs\", len(seq_train_idx), \"Test Seqs\", len(seq_test_idx))\n",
    "#         output_dir = os.path.join(bootstrapping_dir, f\"{date}_{i}.pkl\")\n",
    "#         with open(output_dir, 'wb') as f:\n",
    "#             pickle.dump(all_sequences, f)\n",
    "#             pickle.dump(seq_train_idx, f)\n",
    "#             pickle.dump(seq_test_idx, f)"
   ],
   "id": "1a6bb122f6d0b1ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "dictionary structure:\n",
    " video index(keys) --> sequences (index) --> frames (index) --> nodes & senders & receivers\n",
    " \n",
    "    ## outer loops: video --> multiple sequences --> multiple frames\n",
    "    ## Under one frame: feature numbers, senders and reveicers in all edges.\n",
    "    print(len(V[key]['graph_dicts'][0][0]['nodes']), len(V[key]['graph_dicts'][0][0]['senders']), len(V[key]['graph_dicts'][0][0]['receivers']))\n",
    "    ## number of frames in a sequence\n",
    "    print(len(V[key]['graph_dicts'][0]))\n",
    "    ## number of sequences in a video\n",
    "    print(len(V[key]['graph_dicts']))\n",
    "    \n",
    "  \n",
    "I guess let me try not to divide the sequences first\n",
    "so I would have \n",
    "\n",
    "video index (keys) --> frames(index) --> nodes & senders & receivers\n",
    "\n",
    "I also didn't do bootstrapping"
   ],
   "id": "b9b8252251365b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c80b0cbe9e904c83",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
