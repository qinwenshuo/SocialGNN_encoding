{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Use SocialGNN_encoding conda environment",
   "id": "244d559f73ae0ecc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-28T18:58:43.665209Z",
     "start_time": "2024-08-28T18:58:38.669946Z"
    }
   },
   "source": [
    "from model_functions import SocialGNN, get_inputs_outputs\n",
    "from LSTM_model_functions import CueBasedLSTM, get_inputs_outputs_baseline\n",
    "\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "import os \n",
    "\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:58:52.211929Z",
     "start_time": "2024-08-28T18:58:43.666710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "options = ['LSTM', 'SocialGNN', 'LSTM_Relation']\n",
    "model_selection = input(f'Which model would you like to use? {\", \".join(options)}: ')\n",
    "if model_selection not in options:\n",
    "    raise ValueError(f'Model selection must be in {options}')\n",
    "trained_model_dir = f'TrainedModels/{model_selection}'\n",
    "encoding_output_dir = f'Data/layer_representations/{model_selection}'\n",
    "data_input_dir = 'Data/preprocess/graphs'"
   ],
   "id": "a2a79138e3c90acf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:58:52.227898Z",
     "start_time": "2024-08-28T18:58:52.213846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        pickled = pickle.load(f)\n",
    "    return pickled\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ],
   "id": "a425aeb710fef69b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:58:52.680310Z",
     "start_time": "2024-08-28T18:58:52.229904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "videos = load_pickle(data_input_dir)\n",
    "model_list = sorted(os.listdir(trained_model_dir))\n",
    "sorted_model_list = []\n",
    "\n",
    "if model_selection == 'SocialGNN':\n",
    "    model_config = namedtuple('model_config', 'NUM_NODES NUM_AGENTS V_SPATIAL_SIZE E_SPATIAL_SIZE V_TEMPORAL_SIZE V_OUTPUT_SIZE BATCH_SIZE CLASS_WEIGHTS LEARNING_RATE LAMBDA')\n",
    "    for name in model_list:\n",
    "        if \"SocialGNN_V_5_5_12_12_6_2_20_1.0_1.5_0.001_0.01\" in name:\n",
    "            sorted_model_list.append(name)\n",
    "    for name in model_list:\n",
    "        if \"SocialGNN_V_5_5_12_12_6_5_20_5.69_4.42_1.85_1.66_1.0_0.001_0.01\" in name:\n",
    "            sorted_model_list.append(name)\n",
    "elif model_selection == 'LSTM':\n",
    "    model_config = namedtuple('model_config', 'FEATURE_SIZE V_TEMPORAL_SIZE V_OUTPUT_SIZE BATCH_SIZE CLASS_WEIGHTS LEARNING_RATE LAMBDA')\n",
    "    for name in model_list:\n",
    "        if \"CueBasedLSTM_125_6_2_20_1.0_1.5_0.001_0.01\" in name:\n",
    "            sorted_model_list.append(name)\n",
    "    for name in model_list:\n",
    "        if \"CueBasedLSTM_125_6_5_20_5.69_4.42_1.85_1.66_1.0_0.001_0.01\" in name:\n",
    "            sorted_model_list.append(name)\n",
    "elif model_selection == 'LSTM_Relation':\n",
    "    model_config = namedtuple('model_config', 'FEATURE_SIZE V_TEMPORAL_SIZE V_OUTPUT_SIZE BATCH_SIZE CLASS_WEIGHTS LEARNING_RATE LAMBDA')\n",
    "    for name in model_list:\n",
    "        if 'CueBasedLSTM-Relation_145_6_2_20_1.0_1.5_0.001_0.01' in name:\n",
    "            sorted_model_list.append(name)\n",
    "    for name in model_list:\n",
    "        if 'CueBasedLSTM-Relation_145_6_5_20_5.69_4.42_1.85_1.66_1.0_0.001_0.01' in name:\n",
    "            sorted_model_list.append(name)\n",
    "len(sorted_model_list)"
   ],
   "id": "24ca004a7ca953a9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T19:00:17.796850Z",
     "start_time": "2024-08-28T18:59:26.881109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if model_selection == 'SocialGNN':\n",
    "    old_output_size = None\n",
    "    for model_name in tqdm(sorted_model_list):\n",
    "        model_dir = os.path.join(trained_model_dir, model_name)\n",
    "        if os.path.isdir(model_dir):\n",
    "            parameters = model_name.split('_')[7:]\n",
    "            config = model_config(\n",
    "                NUM_NODES = 5,              # always 5,\n",
    "                NUM_AGENTS = 5,             # always 5\n",
    "                V_SPATIAL_SIZE = 12,        # always 12\n",
    "                E_SPATIAL_SIZE = 12,        # always 12\n",
    "                V_TEMPORAL_SIZE = 6,        # always 6\n",
    "                V_OUTPUT_SIZE = int(parameters[5]),      # 2 or 5\n",
    "                BATCH_SIZE = 10,            # 20, but forced to be 10 so that I don't need to worry about the padded data\n",
    "                CLASS_WEIGHTS = [float(parameters[7+i]) for i in range(int(parameters[5]))],\n",
    "                LEARNING_RATE = 0.001,      # always 0.001\n",
    "                LAMBDA = 0.01               # always 0.01\n",
    "            )\n",
    "            if config.V_OUTPUT_SIZE != old_output_size:\n",
    "                sample_graph_dicts_list, video_timesteps, _ = get_inputs_outputs(videos)\n",
    "                model = SocialGNN(videos, config, sample_graph_dicts_list)\n",
    "                old_output_size = config.V_OUTPUT_SIZE\n",
    "            model.load_model(model_dir)\n",
    "            layer_representations = model.get_layer_representations()\n",
    "            \n",
    "            for key in layer_representations:\n",
    "                output_dir = os.path.join(encoding_output_dir, key, model_name)\n",
    "                save_pickle(layer_representations[key], output_dir)\n",
    "                \n",
    "elif model_selection == 'LSTM':\n",
    "    old_output_size = None\n",
    "    for model_name in tqdm(sorted_model_list):\n",
    "        model_dir = os.path.join(trained_model_dir, model_name)\n",
    "        if os.path.isdir(model_dir):\n",
    "            \n",
    "            parameters = model_name.split('_')[6:]\n",
    "            config = model_config(\n",
    "                FEATURE_SIZE = 125,         # always 125\n",
    "                V_TEMPORAL_SIZE = 6,        # always 6\n",
    "                V_OUTPUT_SIZE = int(parameters[2]),      # 2 or 5\n",
    "                BATCH_SIZE = 10,            # 20, but forced to be 10 so that I don't need to worry about the padded data\n",
    "                CLASS_WEIGHTS = [float(parameters[4+i]) for i in range(int(parameters[2]))],\n",
    "                LEARNING_RATE = 0.001,      # always 0.001\n",
    "                LAMBDA = 0.01               # always 0.01\n",
    "            )\n",
    "    \n",
    "    \n",
    "            if config.V_OUTPUT_SIZE != old_output_size:\n",
    "                # sample_graph_dicts_list, video_timesteps, _ = get_inputs_outputs_baseline(videos)\n",
    "                model = CueBasedLSTM(videos, config, explicit_edges=False)\n",
    "                old_output_size = config.V_OUTPUT_SIZE\n",
    "            \n",
    "            model.load_model(model_dir)\n",
    "            layer_representations = model.get_layer_representations()\n",
    "            \n",
    "            for key in layer_representations:\n",
    "                output_dir = os.path.join(encoding_output_dir, key, model_name)\n",
    "                save_pickle(layer_representations[key], output_dir)  \n",
    "elif model_selection == 'LSTM_Relation':\n",
    "    old_output_size = None\n",
    "    for model_name in tqdm(sorted_model_list):\n",
    "        model_dir = os.path.join(trained_model_dir, model_name)\n",
    "        if os.path.isdir(model_dir):\n",
    "            \n",
    "            parameters = model_name.split('_')[6:]\n",
    "            config = model_config(\n",
    "                FEATURE_SIZE = 145,         # always 145\n",
    "                V_TEMPORAL_SIZE = 6,        # always 6\n",
    "                V_OUTPUT_SIZE = int(parameters[2]),      # 2 or 5\n",
    "                BATCH_SIZE = 10,            # 20, but forced to be 10 so that I don't need to worry about the padded data\n",
    "                CLASS_WEIGHTS = [float(parameters[4+i]) for i in range(int(parameters[2]))],\n",
    "                LEARNING_RATE = 0.001,      # always 0.001\n",
    "                LAMBDA = 0.01               # always 0.01\n",
    "            )\n",
    "    \n",
    "    \n",
    "            if config.V_OUTPUT_SIZE != old_output_size:\n",
    "                # sample_graph_dicts_list, video_timesteps, _ = get_inputs_outputs_baseline(videos)\n",
    "                model = CueBasedLSTM(videos, config, explicit_edges=True)\n",
    "                old_output_size = config.V_OUTPUT_SIZE\n",
    "            \n",
    "            model.load_model(model_dir)\n",
    "            layer_representations = model.get_layer_representations()\n",
    "            \n",
    "            for key in layer_representations:\n",
    "                output_dir = os.path.join(encoding_output_dir, key, model_name)\n",
    "                save_pickle(layer_representations[key], output_dir)  "
   ],
   "id": "832548a4dd3bb547",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............DEFINING INPUT PLACEHOLDERS..............\n",
      "\n",
      ".............BUILDING GRAPH..............\n",
      "\n",
      ".............INITIALIZATION SESSION..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [00:25<00:25,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............DEFINING INPUT PLACEHOLDERS..............\n",
      "\n",
      ".............BUILDING GRAPH..............\n",
      "\n",
      ".............INITIALIZATION SESSION..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:50<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:58:55.182092Z",
     "start_time": "2024-08-28T18:58:55.182092Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6ec86f16573c2443",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
