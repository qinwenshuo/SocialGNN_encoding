{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " Use SocialGNN_encoding conda environment",
   "id": "244d559f73ae0ecc"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-13T16:26:45.662981Z",
     "start_time": "2024-09-13T16:26:42.274320Z"
    }
   },
   "source": [
    "from model_functions import SocialGNN, get_inputs_outputs\n",
    "from LSTM_model_functions import CueBasedLSTM, get_inputs_outputs_baseline\n",
    "\n",
    "from collections import namedtuple\n",
    "import pickle\n",
    "import os \n",
    "\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:26:45.678981Z",
     "start_time": "2024-09-13T16:26:45.662981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "options = ['LSTM', 'SocialGNN', 'LSTM_Relation']\n",
    "data_input_dir = 'Data/preprocess/graphs'"
   ],
   "id": "a2a79138e3c90acf",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:26:45.694977Z",
     "start_time": "2024-09-13T16:26:45.678981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        pickled = pickle.load(f)\n",
    "    return pickled\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ],
   "id": "a425aeb710fef69b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:26:46.053872Z",
     "start_time": "2024-09-13T16:26:45.694977Z"
    }
   },
   "cell_type": "code",
   "source": "videos = load_pickle(data_input_dir)",
   "id": "d601c58f3ece0ef6",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:26:46.070504Z",
     "start_time": "2024-09-13T16:26:46.053872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_parameters(model_selection, trained_model_dir):\n",
    "    model_list = sorted(os.listdir(trained_model_dir))\n",
    "    sorted_model_list = []\n",
    "    if model_selection == 'SocialGNN':\n",
    "        model_config = namedtuple('model_config', 'NUM_NODES NUM_AGENTS V_SPATIAL_SIZE E_SPATIAL_SIZE V_TEMPORAL_SIZE V_OUTPUT_SIZE BATCH_SIZE CLASS_WEIGHTS LEARNING_RATE LAMBDA')\n",
    "        for name in model_list:\n",
    "            if \"SocialGNN_V_5_5_12_12_6_2_20_1.0_1.5_0.001_0.01\" in name:\n",
    "                sorted_model_list.append(name)\n",
    "        for name in model_list:\n",
    "            if \"SocialGNN_V_5_5_12_12_6_5_20_5.69_4.42_1.85_1.66_1.0_0.001_0.01\" in name:\n",
    "                sorted_model_list.append(name)\n",
    "    elif model_selection == 'LSTM':\n",
    "        model_config = namedtuple('model_config', 'FEATURE_SIZE V_TEMPORAL_SIZE V_OUTPUT_SIZE BATCH_SIZE CLASS_WEIGHTS LEARNING_RATE LAMBDA')\n",
    "        for name in model_list:\n",
    "            if \"CueBasedLSTM_125_6_2_20_1.0_1.5_0.001_0.01\" in name:\n",
    "                sorted_model_list.append(name)\n",
    "        for name in model_list:\n",
    "            if \"CueBasedLSTM_125_6_5_20_5.69_4.42_1.85_1.66_1.0_0.001_0.01\" in name:\n",
    "                sorted_model_list.append(name)\n",
    "    elif model_selection == 'LSTM_Relation':\n",
    "        model_config = namedtuple('model_config', 'FEATURE_SIZE V_TEMPORAL_SIZE V_OUTPUT_SIZE BATCH_SIZE CLASS_WEIGHTS LEARNING_RATE LAMBDA')\n",
    "        for name in model_list:\n",
    "            if 'CueBasedLSTM-Relation_145_6_2_20_1.0_1.5_0.001_0.01' in name:\n",
    "                sorted_model_list.append(name)\n",
    "        for name in model_list:\n",
    "            if 'CueBasedLSTM-Relation_145_6_5_20_5.69_4.42_1.85_1.66_1.0_0.001_0.01' in name:\n",
    "                sorted_model_list.append(name)\n",
    "    return model_config, sorted_model_list"
   ],
   "id": "24ca004a7ca953a9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:26:46.086785Z",
     "start_time": "2024-09-13T16:26:46.071753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_layer_representations(model_selection):\n",
    "    trained_model_dir = f'TrainedModels/{model_selection}'\n",
    "    encoding_output_dir = f'Data/layer_representations/{model_selection}'\n",
    "    model_config, sorted_model_list = load_parameters(model_selection, trained_model_dir)\n",
    "    if model_selection == 'SocialGNN':\n",
    "        old_output_size = None\n",
    "        for model_name in tqdm(sorted_model_list):\n",
    "            model_dir = os.path.join(trained_model_dir, model_name)\n",
    "            if os.path.isdir(model_dir):\n",
    "                parameters = model_name.split('_')[7:]\n",
    "                config = model_config(\n",
    "                    NUM_NODES = 5,              # always 5,\n",
    "                    NUM_AGENTS = 5,             # always 5\n",
    "                    V_SPATIAL_SIZE = 12,        # always 12\n",
    "                    E_SPATIAL_SIZE = 12,        # always 12\n",
    "                    V_TEMPORAL_SIZE = 6,        # always 6\n",
    "                    V_OUTPUT_SIZE = int(parameters[5]),      # 2 or 5, only use 5\n",
    "                    BATCH_SIZE = 10,            # 20, but forced to be 10 so that I don't need to worry about the padded data\n",
    "                    CLASS_WEIGHTS = [float(parameters[7+i]) for i in range(int(parameters[5]))],\n",
    "                    LEARNING_RATE = 0.001,      # always 0.001\n",
    "                    LAMBDA = 0.01               # always 0.01\n",
    "                )\n",
    "                if config.V_OUTPUT_SIZE != old_output_size:\n",
    "                    sample_graph_dicts_list, video_timesteps, _ = get_inputs_outputs(videos)\n",
    "                    model = SocialGNN(videos, config, sample_graph_dicts_list)\n",
    "                    old_output_size = config.V_OUTPUT_SIZE\n",
    "                model.load_model(model_dir)\n",
    "                layer_representations = model.get_layer_representations()\n",
    "                \n",
    "                for key in layer_representations:\n",
    "                    output_dir = os.path.join(encoding_output_dir, key, model_name)\n",
    "                    save_pickle(layer_representations[key], output_dir)\n",
    "                    \n",
    "    elif model_selection == 'LSTM':\n",
    "        old_output_size = None\n",
    "        for model_name in tqdm(sorted_model_list):\n",
    "            model_dir = os.path.join(trained_model_dir, model_name)\n",
    "            if os.path.isdir(model_dir):\n",
    "                \n",
    "                parameters = model_name.split('_')[6:]\n",
    "                config = model_config(\n",
    "                    FEATURE_SIZE = 125,         # always 125\n",
    "                    V_TEMPORAL_SIZE = 6,        # always 6\n",
    "                    V_OUTPUT_SIZE = int(parameters[2]),      # 2 or 5\n",
    "                    BATCH_SIZE = 10,            # 20, but forced to be 10 so that I don't need to worry about the padded data\n",
    "                    CLASS_WEIGHTS = [float(parameters[4+i]) for i in range(int(parameters[2]))],\n",
    "                    LEARNING_RATE = 0.001,      # always 0.001\n",
    "                    LAMBDA = 0.01               # always 0.01\n",
    "                )\n",
    "        \n",
    "        \n",
    "                if config.V_OUTPUT_SIZE != old_output_size:\n",
    "                    # sample_graph_dicts_list, video_timesteps, _ = get_inputs_outputs_baseline(videos)\n",
    "                    model = CueBasedLSTM(videos, config, explicit_edges=False)\n",
    "                    old_output_size = config.V_OUTPUT_SIZE\n",
    "                \n",
    "                model.load_model(model_dir)\n",
    "                layer_representations = model.get_layer_representations()\n",
    "                \n",
    "                for key in layer_representations:\n",
    "                    output_dir = os.path.join(encoding_output_dir, key, model_name)\n",
    "                    save_pickle(layer_representations[key], output_dir)  \n",
    "    elif model_selection == 'LSTM_Relation':\n",
    "        old_output_size = None\n",
    "        for model_name in tqdm(sorted_model_list):\n",
    "            model_dir = os.path.join(trained_model_dir, model_name)\n",
    "            if os.path.isdir(model_dir):\n",
    "                \n",
    "                parameters = model_name.split('_')[6:]\n",
    "                config = model_config(\n",
    "                    FEATURE_SIZE = 145,         # always 145\n",
    "                    V_TEMPORAL_SIZE = 6,        # always 6\n",
    "                    V_OUTPUT_SIZE = int(parameters[2]),      # 2 or 5\n",
    "                    BATCH_SIZE = 10,            # 20, but forced to be 10 so that I don't need to worry about the padded data\n",
    "                    CLASS_WEIGHTS = [float(parameters[4+i]) for i in range(int(parameters[2]))],\n",
    "                    LEARNING_RATE = 0.001,      # always 0.001\n",
    "                    LAMBDA = 0.01               # always 0.01\n",
    "                )\n",
    "        \n",
    "        \n",
    "                if config.V_OUTPUT_SIZE != old_output_size:\n",
    "                    # sample_graph_dicts_list, video_timesteps, _ = get_inputs_outputs_baseline(videos)\n",
    "                    model = CueBasedLSTM(videos, config, explicit_edges=True)\n",
    "                    old_output_size = config.V_OUTPUT_SIZE\n",
    "                \n",
    "                model.load_model(model_dir)\n",
    "                layer_representations = model.get_layer_representations()\n",
    "                \n",
    "                for key in layer_representations:\n",
    "                    output_dir = os.path.join(encoding_output_dir, key, model_name)\n",
    "                    save_pickle(layer_representations[key], output_dir)  "
   ],
   "id": "832548a4dd3bb547",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:44:12.761993Z",
     "start_time": "2024-09-13T16:26:46.087389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for model_selection in options:\n",
    "    save_layer_representations(model_selection)"
   ],
   "id": "6ec86f16573c2443",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............DEFINING INPUT PLACEHOLDERS..............\n",
      "\n",
      ".............BUILDING GRAPH..............\n",
      "\n",
      ".............INITIALIZATION SESSION..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [00:15<00:15,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............DEFINING INPUT PLACEHOLDERS..............\n",
      "\n",
      ".............BUILDING GRAPH..............\n",
      "\n",
      ".............INITIALIZATION SESSION..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:30<00:00,  1.29it/s]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............DEFINING INPUT PLACEHOLDERS..............\n",
      "\n",
      ".............BUILDING GRAPH..............\n",
      "\n",
      ".............INITIALIZATION SESSION..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [06:47<04:40, 14.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............DEFINING INPUT PLACEHOLDERS..............\n",
      "\n",
      ".............BUILDING GRAPH..............\n",
      "\n",
      ".............INITIALIZATION SESSION..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [14:26<00:00, 21.67s/it]\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............DEFINING INPUT PLACEHOLDERS..............\n",
      "\n",
      ".............BUILDING GRAPH..............\n",
      "\n",
      ".............INITIALIZATION SESSION..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [01:13<01:11,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............DEFINING INPUT PLACEHOLDERS..............\n",
      "\n",
      ".............BUILDING GRAPH..............\n",
      "\n",
      ".............INITIALIZATION SESSION..............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [02:27<00:00,  3.70s/it]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-13T16:44:12.794002Z",
     "start_time": "2024-09-13T16:44:12.774021Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "41cf3bec8b27f482",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
